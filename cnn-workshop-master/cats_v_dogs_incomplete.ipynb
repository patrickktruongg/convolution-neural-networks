{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop you will be building a Convolutional neural network to classify cats vs dogs. You will need to be familiar with the theory of CNNs. Visit our lesson [here](http://caisplusplus.usc.edu/blog/curriculum/lesson7) for more info. Only fill in the TODO sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports, make sure you have cv2 installed!\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "\n",
    "# DO NOT CHANGE ANY OF THIS\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "TEST_PERCENT = 0.1\n",
    "# This is just for sake of time. In real situations of course you would use the whole dataset.\n",
    "SELECT_SUBSET_PERCENT = 0.15\n",
    "\n",
    "# The cat and dog images are of variable size we have to resize them to all the same size.\n",
    "# DO NOT CHANGE\n",
    "RESIZE_WIDTH=32\n",
    "RESIZE_HEIGHT=32\n",
    "# We are setting this to be 5 epochs for fast training times. In practice we would have many more epochs. \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Load the train and test data sets. Do not modify this code at all. Make sure that your data for cats and dogs images is in ``./data``. You can find that data at https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 3750 files\n",
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Train set has dimensionality (3375, 32, 32, 3)\n",
      "Test set has dimensionality (375, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lets get started by loading the data.\n",
    "# Make sure you have the data downloaded to ./data\n",
    "# To download the data go to https://www.kaggle.com/c/dogs-vs-cats/data and download train.zip\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files = os.listdir(DATA_PATH)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files = sklearn.utils.shuffle(files)\n",
    "select_count = int(len(shuffled_files) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count)\n",
    "\n",
    "subset_files_select = shuffled_files[:select_count]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH + input_file)\n",
    "    # Resize the images to be the same size.\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 'cat' == input_file.split('.')[0]:\n",
    "        Y.append(0.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "While not necessary for this problem you can go ahead and try some preprocessing steps to try to get higher accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO: (Optional)\n",
    "# Perform any data preprocessing steps\n",
    "\n",
    "\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "Here are some useful resources to help with defining a powerful network.\n",
    "- Convolution layers (use the 2D convolution) https://keras.io/layers/convolutional/\n",
    "- Batch norm layer https://keras.io/layers/normalization/\n",
    "- Layer initializers https://keras.io/initializers/\n",
    "- Dense layer https://keras.io/layers/core/#dense\n",
    "- Activation functions https://keras.io/layers/core/#activation\n",
    "- Regulizers: \n",
    "    - https://keras.io/layers/core/#dropout\n",
    "    - https://keras.io/regularizers/\n",
    "    - https://keras.io/callbacks/#earlystopping\n",
    "    - https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "######################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define the network\n",
    "\n",
    "######################################\n",
    "\n",
    "#Convolution Layer 1\n",
    "model.add(Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu')) # Relu activation\n",
    "\n",
    "#Convolution Layer 2\n",
    "model.add(Conv2D(filters=16, kernel_size=(4, 4), strides=(1, 1)))\n",
    "model.add(Activation('relu')) # Relu activation\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#Convolution Layer 3\n",
    "model.add(Conv2D(filters=32, kernel_size=(4, 4), strides=(1, 1)))\n",
    "model.add(Activation('relu')) # Relu activation\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "#Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu')) # Relu activation\n",
    "\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu')) # Relu activation\n",
    "\n",
    "model.add(Dense(units=1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define your loss and your objective\n",
    "optimizer = 'RMSprop'\n",
    "loss = 'binary_crossentropy'\n",
    "######################################\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Time\n",
    "Train the network. Be on the lookout for the validation loss and accuracy. Don't change any of the parameters here except for the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 675 samples\n",
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 4s - loss: 0.1764 - acc: 0.9285 - val_loss: 1.0161 - val_acc: 0.6711\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 4s - loss: 0.1349 - acc: 0.9467 - val_loss: 1.0311 - val_acc: 0.6919\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 4s - loss: 0.1066 - acc: 0.9581 - val_loss: 1.2973 - val_acc: 0.6637\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 4s - loss: 0.0879 - acc: 0.9696 - val_loss: 1.3930 - val_acc: 0.6785\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 4s - loss: 0.0804 - acc: 0.9733 - val_loss: 1.4560 - val_acc: 0.6711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14663694a20>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO:\n",
    "#Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time\n",
    "Now it's time to actually test the network. \n",
    "\n",
    "Get above **65%**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/375 [===========================>..] - ETA: 0s\n",
      "Got 67.20% accuracy\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
